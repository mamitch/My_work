{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Metrics\n",
    "## More potatoes more problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](img/canada-potato-harvest-2018.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we get them out of the dirt?\n",
    "![img2](img/planting-manitoba.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<blockquote class=\"embedly-card\"><h4><a href=\"https://link.springer.com/article/10.1007%2Fs11694-018-9943-9\">Separating clods and stones from potato tubers based on color and shape</a></h4><p>The separation of clods and stones from the harvested potato tuber has always been a prevalent problem in the world. However, the precision of sorting was restricted by the potato tubers covered with...</p></blockquote><script async src=\"//cdn.embedly.com/widgets/platform.js\" charset=\"UTF-8\"></script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/R5p1XmnTUuk\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/R5p1XmnTUuk\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have a dataset from the Potato Industry and Research Forum, **PIRF**.<br>\n",
    "\n",
    "**Our task**:<br>\n",
    "Find the best cuttoff to discern between potato and clod of dirt so they can sort their harvest effectively.\n",
    "- classify objects based on a measurement\n",
    "- evaluate that cutoff\n",
    "- choose the best cutoff based on a cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=11)\n",
    "measures = np.round(np.random.uniform(1.5,10,100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1886)\n",
    "potato = [1, 1, 0]\n",
    "potato_list = np.random.choice(potato, replace = True, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_dataset = pd.DataFrame({'measure': list(np.sort(measures)), 'potato': list(potato_list)}, columns=['measure', 'potato'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Quick summary stats\n",
    "Let's take a quick count of some important potato-related numbers before we move forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(machine_dataset)\n",
    "true_potatoes = sum(machine_dataset.potato)\n",
    "true_potatoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Picking a cut-off to sort potatoes vs clods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_dataset['label'] = np.where(machine_dataset['measure'] >= 5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Okay, but is 5 the **right** cut off? How do we know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix!!\n",
    "\n",
    "![confu](img/images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's calculate some NUMBERS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positives \n",
    "machine_dataset['TP'] = np.where((machine_dataset['potato']==1) & (machine_dataset['label']==1), 1, 0)\n",
    "machine_dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives \n",
    "machine_dataset['FP'] = np.where((machine_dataset['potato']==0) & (machine_dataset['label']==1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Negatives \n",
    "machine_dataset['TN'] = np.where((machine_dataset['potato']==0) & (machine_dataset['label']==0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negatives\n",
    "machine_dataset['FN'] = np.where((machine_dataset['potato']==1) & (machine_dataset['label']==0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_dataset.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's put some numbers in CONTEXT!!!!\n",
    "\n",
    "\n",
    "\n",
    "![](./img/confusion_matrix.jpg)\n",
    "![imgc2](./img/tpr_fpr.png)\n",
    "\n",
    "#### Sensitivity=Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive Rate (Sensitivity, Recall)\n",
    "# TP/(all true positives)\n",
    "truep = machine_dataset.TP.sum()/(machine_dataset.TP.sum()+machine_dataset.FN.sum())\n",
    "\n",
    "\n",
    "# True Negative Rate (Specificity)\n",
    "# TN/(all true negatives)\n",
    "spec = machine_dataset.TN.sum()/(machine_dataset.TN.sum()+machine_dataset.FP.sum())\n",
    "\n",
    "# False Positive Rate \n",
    "# FP/(All true negatives) or 1-Specificity\n",
    "fpr = 1-spec\n",
    "\n",
    "print(truep, spec, fpr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation!\n",
    "\n",
    "TPR- Of all the actual potatoes our cut score correctly classified 71% of them.\n",
    "\n",
    "Specificity-  Of all the actual dirt clods (aka, not potatoes) our cut score correctly classified 63% of them.\n",
    "\n",
    "FPR- Of all the actual dirt clods our cut score incorrectly classified 38% of them as potatoes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But wait!!!\n",
    "\n",
    "![easier](img/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(machine_dataset.potato, machine_dataset.label)\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = recall_score(machine_dataset.potato, machine_dataset.label)\n",
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get the other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(machine_dataset.potato, machine_dataset.label).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr= fp/(tn+fp)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But do we yet know if a cut off of 5 is the best one to use?\n",
    "\n",
    "![think](img/200w.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Your Turn:\n",
    "\n",
    "Take a different cutoff value:\n",
    "- create a confusion matrix\n",
    "- get the tpr, fpr, tnr, and fnr for your cutoff\n",
    "- compare with a neighbor\n",
    "- whose is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need a way to compare *all* the possible cut-offs.<br>\n",
    "Let's build this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small function to go through a range of float values\n",
    "def frange(start, stop, step):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get in it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['cut_off', 'fpr', 'tpr']\n",
    "potato_metrics = pd.DataFrame()\n",
    "\n",
    "# for loop to test different cut-off values between 2 and 9.5 by steps of .25\n",
    "for i in frange(2,9.5,0.25):\n",
    "    machine_dataset['label'] = np.where(machine_dataset['measure'] >= i, 1, 0)\n",
    "    tn, fp, fn, tp = confusion_matrix(machine_dataset.potato, machine_dataset.label).ravel()\n",
    "    fpr = fp/(tn+fp)\n",
    "    tpr = tp/(tp+fn)\n",
    "    evals = [i,fpr,tpr]\n",
    "    potato_metrics=potato_metrics.append(pd.Series(evals), ignore_index=True)\n",
    "\n",
    "potato_metrics.columns = names       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "potato_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's plot them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# plot the data\n",
    "plt.plot(potato_metrics.fpr, potato_metrics.tpr , 'o', color='black')\n",
    "\n",
    "# add a reference line\n",
    "plt.plot( [0,1],[0,1] )\n",
    "\n",
    "# make some nice labels\n",
    "plt.title('ROC curve')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But still...\n",
    "Where is the ideal point? Which cutoff is the best?<br>\n",
    "\n",
    "Many coding languages and libraries default to selecting the cutoff with the highest _Accuracy_\n",
    "\n",
    "\\begin{equation*}\n",
    "Accuracy = \\frac{TP+TN}{Total} \n",
    "\\end{equation*}\n",
    "\n",
    "It has its own `metric` in sklearn `accuracy_score`.<br>\n",
    "But I believe that's not a fully accurate picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ac = accuracy_score(machine_dataset.potato, machine_dataset.label)\n",
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How else could you get it?\n",
    "Well, it depends on the cost of getting something wrong.\n",
    "\n",
    "\\begin{equation*}\n",
    "Cost = C_0 +  C_{TP}P(TP)+ C_{FP}P(FP) +C_{TN}P(TN) + C_{FN}P(FN) \n",
    "\\end{equation*}\n",
    "\n",
    "How can you minimize the cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Your Turn:\n",
    "\n",
    "Let's assume for every item that goes through the machine has a wear and tear cost of 5 cents <br>\n",
    "Every potato they correctly classify as a potato is gains them 50 cents <br>\n",
    "Every clod of dirt that gets into a bag of potatoes at the grocery store costs them a dollar <br>\n",
    "Every clod of dirt that correctly gets thrown out costs them nothing.<br>\n",
    "Every potato incorrectly classified as dirt looses them 10 cents.\n",
    "\n",
    "Which cutoff is the best for these costs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's adapt a function we used before to find out<br>\n",
    "Finish the formula for `icalc` before running the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['cut_off', 'cost']\n",
    "cost_op = pd.DataFrame()\n",
    "\n",
    "# for loop to test different cut-off values\n",
    "for i in frange(2,9.5,0.25):\n",
    "    machine_dataset['label'] = np.where(machine_dataset['measure'] >= i, 1, 0)\n",
    "    obs = len(machine_dataset)\n",
    "    tn, fp, fn, tp = confusion_matrix(machine_dataset.potato, machine_dataset.label).ravel()\n",
    "    # finish the formula for icalc\n",
    "    icalc = #Your CODE\n",
    "    evals = [i, icalc]\n",
    "    cost_op=cost_op.append(pd.Series(evals), ignore_index=True)\n",
    "\n",
    "cost_op.columns = names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get the minimum cost with it's cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_op[cost_op.cost == cost_op.cost.min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### An even mathier way\n",
    "\n",
    "Now if you want to use **calculus**...\n",
    "\n",
    "Metz (1978) showed that the slope of the ROC curve at the optimal cutoff value is\n",
    "\n",
    "\\begin{equation*}\n",
    "m = \\frac{1 - Prevalence}{Prevalence} x\n",
    "\\frac{C_{FP}-C_{TN}}{C_{FN}-C_{TP}}\n",
    "\\end{equation*}\n",
    "\n",
    "Zweig and Campbell (1993) showed that the point along the ROC curve where the average cost is minimum\n",
    "corresponds to the cutoff value where\n",
    "\n",
    "\\begin{equation*}\n",
    "f_m = sensitivity − m(1 − specificity)\n",
    "\\end{equation*}\n",
    "Where m is slope and fm is the cost function is maximized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Other useful metrics\n",
    "\n",
    "\\begin{equation*}\n",
    "Prevalence = \\frac{Total_{Positives}}{Total_{Observations}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "ps = precision_score(machine_dataset.potato, machine_dataset.label)\n",
    "recall =recall_score(machine_dataset.potato, machine_dataset.label)\n",
    "ps, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(machine_dataset.potato, machine_dataset.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So how else are those curve things used?\n",
    "\n",
    "Used to compare overall performance for different models.  Visualization of the True Positive Rate and the False Positive Rate for all possible thresholds for a given classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![roc](img/roccomp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### ROC (Receiver Operating Characteristic) Curve the real way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some functions\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "fpr, tpr = roc_curve(machine_dataset.potato, machine_dataset.measure)[:2]\n",
    "auc = roc_auc_score(machine_dataset.potato, machine_dataset.measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(machine_dataset.potato, machine_dataset.measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wait, AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Area Under the Curve\n",
    "![auc](img/AUC.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### AUC explained\n",
    "- AUC is a way to quantify and compare ROC curves without having to visually look at each one\n",
    "- A metric that summarizes the ROC curve in a single number\n",
    "- Values range from 0(worst)-1(best)\n",
    "- Values of .5 indicates random chance prediction\n",
    "- For imbalanced problems AUC is often a more meaningful metric than accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scenario Revisted\n",
    "![aussie](img/aussie-new-grading-machine.jpg) \n",
    "\n",
    "Well, we've helped out our potato farming friends! What would need to happen if the costs of misclassifying a potato changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Closing reflection:\n",
    "\n",
    "Identify another scenario where the costs of a FP or FN would greatly impact the ideal cutoff? What about the cost of the test?\n",
    "\n",
    "What if we have more than one variable we need to consider in order to classify observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
