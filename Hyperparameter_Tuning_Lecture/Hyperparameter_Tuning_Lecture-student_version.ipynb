{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Objectives:\n",
    "- Understand the difference between parameters and hyperparameters\n",
    "- Understand why we tune hyperparameters\n",
    "- Understand how hyperparameter tuning impacts the outcomes of your model\n",
    "- Apply hyperparameter tuning to your model using gridsearch and randomsearch\n",
    "- Select the \"best\" hyperparameters for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters vs Hyperparameters\n",
    "\n",
    "We talk a lot about parameters and hyperparameters, but what are they?  What is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/meetupconsumercreditdefaultvers2all-11-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> Why do we tune hyperparameters?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We tune hyperparameters to find the set of hyperparameters that optimize the predictive ability (ex. accuracy, recall, r-squared value, RMSE, etc.)\n",
    "\n",
    "A big factor in whether a machine learning model will perform well is a lot of tweaking..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Pile of data to stir (https://xkcd.com/1838/)](images/machine_learning_xkcd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can think of hyperparameters as little dials to adjust to make it easier for the machine learning model to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/dials.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But how do we know what to adjust them to?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this fit into the CRISP-DM Model?\n",
    "\n",
    "Let's remind ourselves about the steps of the CRISP-DM Model.\n",
    "\n",
    "![](./images/new_crisp-dm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's talk data!\n",
    "\n",
    "The [data](https://www.kaggle.com/jolasa/bay-area-bike-sharing-trips) we will be using today is taken from Kaggle. This dataset contains anonymized trips data of Lyft bike sharing system(Bay Wheels), in the Bay Area from January 2019 to May 2019.\n",
    "\n",
    "\n",
    "We are going to use this data to create a model the predicts if the the member is a \"brogrammer\".  Everyone who is a male and lives in SF AND rides an electric bike is a Brogrammer.  This target has already been created for you.\n",
    "\n",
    "![bikes](images/bikes.jpg) \n",
    "![the wiggle](images/wiggle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read in the data and do some EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike=pd.read_pickle('./data/bike.pkl')\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.bg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.bg.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.user_type_Customer.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.user_type_Subscriber.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.trip_duration_min.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike.member_birth_year.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> What other visualizations could/should we create?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with a Baseline/Dummy Model\n",
    "\n",
    "First we are going to start with a dummy model to predict if the user was a brogrammer.  In our dummy model we classify everything as the majority class.  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random state for our notebook\n",
    "import numpy as np\n",
    "np.random.seed(107)\n",
    "\n",
    "#set brogrammer as the target and use all other features as \n",
    "y = bike['bg']\n",
    "#set features to all variables except brogrammer and trip in seconds\n",
    "X = bike.drop(['bg', 'trip_duration_sec'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split our data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out the shape of our data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> What does the \"score\" from the dummy classifier tell us about the predictive quality of our model?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score from the classifier tells us that 99% of our training data was correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now  let's create a nice looking confusion matrix using the yellowbrick package\n",
    "[yellowbrick documentation](https://www.scikit-yb.org/en/latest/api/classifier/confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(dummy)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> What does this confusion matrix tell us about what the dummy classifier did?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy classifier classified all test data as brogrammer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn: Fitting a Vanilla Model\n",
    "\n",
    "Now that we know what our dummy/baseline classifier does we are going to fit a vanilla decision tree model to predict if the user was a brogrammer.\n",
    "\n",
    "Do the following in your group then answer the questions:\n",
    "\n",
    "1. Create a vanilla decision tree model and fit it to your training data\n",
    "2. Print the accuracy score of this prediction\n",
    "3. Print a confusion matrix for your training data\n",
    "4. Print a classification report for the model\n",
    "\n",
    "**BONUS**:  Repeat the above steps using a Naive Bayes classifier. Compare and contrast the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> 1. What does the \"score\" from the decision tree classifier tell us about the predictive quality of our model?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> 2. According to confusion matrix, how well did our decision tree classifier do?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\">3. According to classification report, how well did our decision tree classifier do?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T20:52:52.092683Z",
     "start_time": "2019-11-07T20:52:52.090712Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Now onto Grid Search: Find the best hyperparameters for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A way for us to search over multiple hyperparameters for the given model to see if we can increase the accuracy of our model by \"tuning\" our hyperparameters in our decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we set up a grid of hyperparameters we want to \"search over\"\n",
    "param_grid = {\n",
    "    'max_depth': [2, 5],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'min_samples_split': [8, 10],\n",
    "}\n",
    "\n",
    "#then we conduct the \"search\"\n",
    "gs = GridSearchCV(estimator = clf_DT, param_grid = param_grid, \n",
    "                          cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "#finally we find the best hyperparameters\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  WOW that took a while!!  Is there an easier way than just waiting for this to run?\n",
    "\n",
    "## Luckily there is!  Let's check out the RandomizedSearchCV feature in sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use the same exact param_grid we have already defined\n",
    "param_grid = {\n",
    "    'max_depth': [2, 5],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'min_samples_split': [8, 10],\n",
    "}\n",
    "\n",
    "#then we conduct the \"search\"\n",
    "rs = RandomizedSearchCV(estimator = clf_DT,  param_distributions = param_grid, \n",
    "                          cv = 5)\n",
    "rs.fit(X_train, y_train)\n",
    "#finally we find the best hyperparameters\n",
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, that ran a bit faster.  But wait...why are our best params different now????\n",
    "\n",
    "In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting our Decision Tree Using Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our best model from the grid search\n",
    "best_DT=gs.best_estimator_\n",
    "\n",
    "#fit the data using the best model and look at the score given test data\n",
    "best_DT.fit(X_train, y_train)\n",
    "best_DT.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> Our vanilla model had an accuracy score of 99%.  How did our tuned model do in comparison?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our tuned model actually did worse than our vanilla model in overall accuracy. But it likely now isn't overfitting our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(best_DT)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> According to confusion matrix, how well did our decision tree classifier do?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this confusion matrix is the same as our dummy classifier! That's not what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=best_DT.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> According to classification report, how well did our decision tree classifier do?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this confirms that we essentially went back to the dummy model where we classified everything as the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines and hyperparameter searching!\n",
    "\n",
    "Yes, let's go back to that concept of a pipeline.  We established using pipelines is important when we our transforming our data and using cross validation (which our gridsearch and randomized search are using).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our pipe, even though ss isn't necessary here let's add it \n",
    "#just to see how the pipe works\n",
    "pipe = make_pipeline(StandardScaler(), DecisionTreeClassifier())\n",
    "\n",
    "\n",
    "#we can use the same exact param_grid we have already defined\n",
    "param_grid = {\n",
    "    'decisiontreeclassifier__max_depth': [2, 5],\n",
    "    'decisiontreeclassifier__max_features': [2, 3],\n",
    "    'decisiontreeclassifier__min_samples_leaf': [3, 4],\n",
    "    'decisiontreeclassifier__min_samples_split': [8, 10],\n",
    "}\n",
    "\n",
    "#now use our pipe in our grid search\n",
    "gridsearch_pipe = GridSearchCV(pipe, \n",
    "                          param_grid = param_grid,\n",
    "                          cv = 5)\n",
    "\n",
    "gridsearch_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our best model from the grid search\n",
    "best_pipe=gridsearch_pipe.best_estimator_\n",
    "\n",
    "#fit the data using the best model and look at the score given test data\n",
    "best_pipe.fit(X_train, y_train)\n",
    "best_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(best_pipe)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> Does this mean that hyperparameter tuning isn't good?  What can we do now?</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lots of options to move forward and increase our classification.  We can:\n",
    "- **try a different classification model**\n",
    "- **tune other hyperparameters in our decision tree model**\n",
    "- **try balancing our classes**\n",
    "- add more data\n",
    "- add new features- maybe we don't have ones that are very predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"color:red\"> Your Turn:  Now work in small groups to take one of the above next steps to increase the classification of brogrammers! </span></font>\n",
    "\n",
    "**When you have selected your VERY BEST Model then use the test data to see how it did!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
