{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubqG8p0ezqE7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "### Let's get to the most important parts of this data\n",
    "\n",
    "![](./img/PCA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting Off\n",
    "\n",
    "Currently when you have a lot of potential features to use in a model, how are you deciding which features to use in your final model, both the number and the specific features?\n",
    "\n",
    "What are some advantages and disadvantages of this approach?\n",
    "\n",
    "And how on earth do you handle mutlicolinearity in a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "By the end of this lesson students will be able to:\n",
    "- Describe PCA and its role in modeling\n",
    "- Apply PCA to datasets\n",
    "- Understand how to select the relevant components to keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Let's demo a concept with this first](https://www.desmos.com/calculator/tpahiv1skb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZuDcXqrzqE8"
   },
   "source": [
    "Up until now, we have been looking in depth at supervised learning estimators: those estimators that predict labels based on labeled training data.\n",
    "\n",
    "![](./img/machine_learning.png)\n",
    "\n",
    "In this section, we explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA).\n",
    "PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.\n",
    "After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "As the dimensionality of the feature space increases, the number of configurations can grow exponentially, and the number of configurations covered by an observation decreases. \n",
    "\n",
    "As we add more dimensions we also increase the processing power we need to analyze the data, and we also increase the amount of training data required to make meaningful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-d Space\n",
    "\n",
    "![1d](https://cdn-images-1.medium.com/max/1600/1*V4g_LzDA0QhWrVbRJqSNLQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2d Space\n",
    "![2d](https://cdn-images-1.medium.com/max/1600/1*m5i7BqDLiZ8t1tyQx8xWHQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3d Space\n",
    "\n",
    "![3d](https://cdn-images-1.medium.com/max/1600/1*sKj4h8S1DRV7Phf22ycAbg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As the number of dimensions increases what happens to the nubmer of observations needed to cover that space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hughes Phenomenon\n",
    "\n",
    "Hughes Phenomenon shows that as the number of features increases, the classifier’s performance increases as well until we reach the optimal number of features. Adding more features based on the same size as the training set will then degrade the classifier’s performance.\n",
    "\n",
    "![hughes](https://cdn-images-1.medium.com/max/1600/1*Ts2X2ow29QLDEeLvSE14Ew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGKDdb3pzqFA"
   },
   "source": [
    "## Introducing Principal Component Analysis\n",
    "\n",
    "Principal component analysis is a fast and flexible **unsupervised method** for **dimensionality reduction** in data.\n",
    "Its behavior is easiest to visualize by looking at a two-dimensional dataset.\n",
    "Consider the following 200 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esewHp_0zqE8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTDjyCd7zqFA",
    "outputId": "0283bea0-446d-4bc0-f741-bb17aaf15a76"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xayl0jMfzqFE"
   },
   "source": [
    "By eye, it is clear that there is a nearly linear relationship between the x and y variables.\n",
    "The problem setting here is slightly different: rather than attempting to *predict* the y values from the x values, the unsupervised learning problem attempts to learn about the *relationship* between the x and y values.\n",
    "\n",
    "In principal component analysis, this relationship is quantified by finding a list of the *principal axes* in the data, and using those axes to describe the dataset.\n",
    "Using Scikit-Learn's ``PCA`` estimator, we can compute this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pIu2Dv7hzqFF",
    "outputId": "61b9c7a6-013d-483c-d549-08781401f1ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPVijTCFzqFH"
   },
   "source": [
    "The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xt_dzib9zqFI",
    "outputId": "44c69b2d-9202-4a71-8838-2450198d4176"
   },
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LbTH8rozqFM",
    "outputId": "f6d1b945-31a4-4de3-daf7-4013ca80ead8"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4piNngUczqFO"
   },
   "source": [
    "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olPtnLAXzqFP",
    "outputId": "c3796079-e7a4-4b78-8fe0-05fb6af73275"
   },
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=3,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyEJyX2AzqFR"
   },
   "source": [
    "These vectors represent the *principal axes* of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis.\n",
    "The projection of each data point onto the principal axes are the \"principal components\" of the data.\n",
    "\n",
    "The 1st component is always a vector that contains the most information(i.e. that explains the most variance in the data).\n",
    "\n",
    "The 2nd principal component is the vector which contains the 2nd most information while being orthoganal (at a right angle) to the first component.\n",
    "\n",
    "If we plot these principal components beside the original data, we see the plots shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uP0n8Hh6zqFS"
   },
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.09-PCA-rotation.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eWbeAppzqFS"
   },
   "source": [
    "This transformation from data axes to principal axes is an *affine transformation*, which basically means it is composed of a translation, rotation, and uniform scaling.\n",
    "\n",
    "While this algorithm to find principal components may seem like just a mathematical curiosity, it turns out to have very far-reaching applications in the world of machine learning and data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep Dive on how the axes is determined](https://www.youtube.com/watch?v=_UVHneBUBW0)\n",
    "\n",
    "[The linear algebra intuition behind this](https://www.youtube.com/watch?v=PFDu9oVAE-g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6BRmJjGzqFS"
   },
   "source": [
    "### PCA as dimensionality reduction\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "Here is an example of using PCA as a dimensionality reduction transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQLuZlrzzqFT",
    "outputId": "76c9bf17-7075-451e-ca39-19886d1e0447"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lzhntUTzqFV"
   },
   "source": [
    "The transformed data has been reduced to a single dimension.\n",
    "To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ph0MjpTHzqFW",
    "outputId": "cc3765a7-572d-4333-ba90-abad8f3088a9"
   },
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgRLviaMzqFY"
   },
   "source": [
    "The light points are the original data, while the dark points are the projected version.\n",
    "This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\n",
    "The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another view of what happened\n",
    "![pca](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tU-5IIqzqFZ"
   },
   "source": [
    "### PCA for visualization: Hand-written digits\n",
    "\n",
    "The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data.\n",
    "To see this, let's take a quick look at the application of PCA to the digits data .\n",
    "\n",
    "We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtaacUC3zqFa",
    "outputId": "0f2bb79b-7b21-49da-a0a7-3a2f9b172505"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits['target'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits['data'][4].reshape((8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(digits['data'][4].reshape((8,8)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qf1rE3OAzqFd"
   },
   "source": [
    "The data consists of 8×8 pixel images, meaning that they are 64-dimensional.\n",
    "To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsRwQvdMzqFd",
    "outputId": "08a1ae2e-f44c-4194-890f-b75cffea8418"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `explained_variance_` attribute we can examine the amount of variance explained by each of the selected components. \n",
    "\n",
    "\n",
    "The `explained_variance_ratio_` attribute tells us the percentage of variance explained by each of the selected components.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "theKjXZhzqFg"
   },
   "source": [
    "We can now plot the first two principal components of each point to learn about the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "cmap = mpl.colors.ListedColormap(['red', 'green', 'blue', 'yellow', 'purple', 'cyan',\n",
    "                                  'orange', 'pink', 'brown', 'black', 'teal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeAb8lmJzqFh",
    "outputId": "3c78698b-bb5c-4d80-a56f-52563f01bb47"
   },
   "outputs": [],
   "source": [
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=cmap\n",
    "            )\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "enQFGYj8zqFk"
   },
   "source": [
    "Recall what these components mean: the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance.\n",
    "Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an **unsupervised manner**—that is, **without reference to the labels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uoqz1-gazqFr"
   },
   "source": [
    "### Choosing the number of components\n",
    "\n",
    "A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data.  Two of the most common ways to approach the selection of components are the following:\n",
    "\n",
    "1.  Select a number of components that explain a sufficiently large proportion of variance (e.g., 80 % of the variance)\n",
    "\n",
    "2. Examine a plot of the explained variance and look for an \"elbow\" in the curve where the explained variance stops growing fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DnAGsCnUzqFs",
    "outputId": "a55837a7-a33c-43e3-9f91-86ae81d79b21"
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vZmzc7izqFu"
   },
   "source": [
    "This curve quantifies how much of the total, 64-dimensional variance is contained within the first $N$ components.\n",
    "For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.\n",
    "\n",
    "Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we'd need about 20 components to retain 90% of the variance.  Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[:30].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can run another PCA model this time extracting the components such that the amount of variance that needs to be explained is greater 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.80)\n",
    "pca.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_tvkE-DzqFv"
   },
   "source": [
    "## PCA as Noise Filtering\n",
    "\n",
    "PCA can also be used as a filtering approach for noisy data.\n",
    "The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise.\n",
    "So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.\n",
    "\n",
    "Let's see how this looks with the digits data.\n",
    "First we will plot several of the input noise-free data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYwp6c0ezqFv",
    "outputId": "87310218-30c2-4991-dfe5-69a9ac3f7cfc"
   },
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAnMZ59kzqFx"
   },
   "source": [
    "Now lets add some random noise to create a noisy dataset, and re-plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-rToaInzqFy",
    "outputId": "46f15f6c-676c-4266-9c98-c8fb10413bd0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "noisy = np.random.normal(digits.data, 4)\n",
    "plot_digits(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBzxvvN6zqF2"
   },
   "source": [
    "It's clear by eye that the images are noisy, and contain spurious pixels.\n",
    "Let's train a PCA on the noisy data, requesting that the projection preserve 50% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrUyJK5zzqF2",
    "outputId": "fe27b064-299d-4dff-a5a0-05a078048af4"
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(noisy)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[:12].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.5).fit(noisy)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-pbFinszqF4"
   },
   "source": [
    "Here 50% of the variance amounts to 12 principal components.\n",
    "Now we compute these components, and then use the inverse of the transform to reconstruct the filtered digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J874pfAqzqF5",
    "outputId": "6a67d537-1295-41bc-ef89-07cc25fec745"
   },
   "outputs": [],
   "source": [
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using components for modeling\n",
    "Now we can use our 12 best components to classify our digits using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pca = PCA(12)\n",
    "X_components = pca.fit_transform(digits.data)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_components, digits.target)\n",
    "lr.score(X_components, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the original data\n",
    "X = digits.data\n",
    "lr.fit(X, digits.target)\n",
    "lr.score(X, digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_qThADFzqF8"
   },
   "source": [
    "This signal preserving/noise filtering property makes PCA a very useful feature selection routine—for example, rather than training a classifier on very high-dimensional data, you might instead train the classifier on the lower-dimensional representation, which will automatically serve to filter out random noise in the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsfWc-5_zqGM"
   },
   "source": [
    "## Principal Component Analysis Summary\n",
    "\n",
    "We have discussed the use of principal component analysis for dimensionality reduction, for visualization of high-dimensional data, for noise filtering, and for feature selection within high-dimensional data.\n",
    "\n",
    "### Strengths\n",
    "- Effective in a wide variety of contexts and disciplines.\n",
    "- Offers a straightforward and efficient path to gaining insight into high-dimensional data.\n",
    "- Can reduce overfitting.\n",
    "- Removed highly correlated variables.\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- Highly affected by outliers in the data.\n",
    "- Newly created components lose their interpretability.\n",
    "- You must standardize your data before fitting PCA.\n",
    "\n",
    "\n",
    "### Other variations\n",
    "Scikit-Learn contains a couple interesting variants on PCA, including ``RandomizedPCA`` and ``SparsePCA``, both also in the ``sklearn.decomposition`` submodule.\n",
    "``RandomizedPCA``, which we saw earlier, uses a non-deterministic method to quickly approximate the first few principal components in very high-dimensional data, while ``SparsePCA`` introduces a regularization term (see [In Depth: Linear Regression](05.06-Linear-Regression.ipynb)) that serves to enforce sparsity of the components.\n",
    "\n",
    "In the following sections, we will look at other unsupervised learning methods that build on some of the ideas of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!!  Practice on new data set\n",
    "\n",
    "Let's practice PCA on the [glass dataset](https://raw.githubusercontent.com/p-sama/Glass-Classification/master/glass.csv).  \n",
    "\n",
    "\n",
    "The columns in this dataset are:\n",
    "\n",
    "- RI: refractive index\n",
    "- Na: Sodium\n",
    "- Mg: Magnesium\n",
    "- Al: Aluminum\n",
    "- Si: Silica\n",
    "- K: Potassium\n",
    "- Ca: Calcium\n",
    "- Ba: Barium\n",
    "- Fe: Iron\n",
    "- Type of glass (Target label), values 1-6 below:\n",
    "1. building_windows_float_processed\n",
    "2. building_windows_non_float_processed\n",
    "3. vehicle_windows_float_processed\n",
    "4. containers\n",
    "5. tableware\n",
    "6. headlamps\n",
    "\n",
    "\n",
    "\n",
    "**Complete the following with your group.**\n",
    "\n",
    "\n",
    "1. Run a random forest classifier with `max_depth`=5  on the original dataset and print out appropriate classification metrics\n",
    "2. Conduct PCA to reduce the dimensionality of your features.  Determine the number of features using an appropriate method as discussed in today's lecture.\n",
    "3. Re-run your random forest classifier using your components.\n",
    "4. Compare the models.  Were you more \"successful\" in your predictions with the reduced features?\n",
    "5. **BONUS**: Use a pipeline to perform the preprocessing steps as well as the modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Additional Resources\n",
    "\n",
    "[A detailed presentation on PCA](http://luthuli.cs.uiuc.edu/~daf/courses/CS-498-DAF-PS/Lecture%209%20-%20PCA.pdf)\n",
    "\n",
    "[Interactive tool for PCA](http://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "[For noise cancellation example](https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_digits_simple_classif.html)\n",
    "\n",
    "[Kernel PCA vs PCA vs ICA](https://towardsdatascience.com/kernel-pca-vs-pca-vs-ica-in-tensorflow-sklearn-60e17eb15a64)\n",
    "\n",
    "[Comparison of PCA and KPCA](https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py)\n",
    "\n",
    "[Principal Component Analysis in 3 Simple Steps](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#1---eigendecomposition---computing-eigenvectors-and-eigenvalues)\n",
    "\n",
    "[How to compute PCA Loadings and the Loading Matrix with scikit-learn](https://scentellegher.github.io/machine-learning/2020/01/27/pca-loadings-sklearn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "b6BRmJjGzqFS",
    "6tU-5IIqzqFZ",
    "jDO3nDlXzqFk",
    "Uoqz1-gazqFr"
   ],
   "name": "Copy of 05.09-Principal-Component-Analysis.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb",
     "timestamp": 1557945856916
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
