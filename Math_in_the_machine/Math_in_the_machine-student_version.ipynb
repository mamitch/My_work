{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Math in The Machine\n",
    "\n",
    "<img src= \"./resources/muchmath.png\" style = \"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "By the end of class students will be able to:\n",
    "- summarize linear algebra in linear regression \n",
    "- define challenges of finding coefficient values\n",
    "- describe how gradient decent and cost functions saves the day\n",
    " - define the *gradient* in gradient descent (hint, it's slope)\n",
    " - define the *descent* in gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with...\n",
    "<img src= \"./resources/algebra.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebra's from highschool, but that doesn't make it simple\n",
    "\n",
    "#### Problem 1:\n",
    "Solve for $x$</br>\n",
    "\n",
    "$20  = 5 + 3x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2:\n",
    "Solve for $x$</br>\n",
    "\n",
    "$20 - 7x = 6x - 6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    "Solve for $x$ and $y$</br>\n",
    "\n",
    "$-2(x - 1) + 4y = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4\n",
    "Solve for $x_1$ and $x_2$</br>\n",
    "\n",
    "$4x_1 + 2x_2 = 8$</br>\n",
    "\n",
    "$5x_1 + 3x_2 = 9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are all these problems doing?\n",
    "\n",
    "solving for the _unknown_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of magnitude\n",
    "\n",
    "Now Problem 4 might be doable by hand, but what if instead of 2 equations we had 5? 20? 200? 50,000?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fortunately for us we have\n",
    "\n",
    "<img src= \"https://media0.giphy.com/media/JlxFcvNuzlPYA/giphy.gif?cid=790b7611c4a4fc74c05cd06fe2c8cc00860e04b6f8049e52&rid=giphy.gif\">\n",
    "\n",
    "## Computers!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But there is a problem:\n",
    "\n",
    "| people | computers|\n",
    "|--------|----------|\n",
    "|can read equations like sentences | can't really do that |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear albegra solves that problem, by turning this:\n",
    "\n",
    "$4x_1 + 2x_2 = 8$</br>\n",
    "\n",
    "$5x_1 + 3x_2 = 9$\n",
    "\n",
    "### into this:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}4 & 2 \\\\ 5 & 3 \\end{bmatrix}*\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} = \\begin{bmatrix}8\\\\9\\end{bmatrix}\n",
    "$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise, how would we rewrite the equation sets in each problem into linear algebra?\n",
    "\n",
    "#### Problem 1\n",
    "\n",
    "$x_0 + 2x_1 = 10$\n",
    "</br>\n",
    "\n",
    "$3x_0 + x_1 = 9$\n",
    "\n",
    "#### Problem 2\n",
    "\n",
    "$x_0 + 2x_1 = 10$</br>\n",
    "\n",
    "$3x_0 + x_1 = 9$</br>\n",
    "\n",
    "$32x_0 - 6x_1 = 24$\n",
    "\n",
    "#### Problem 3\n",
    "$x_0 + 2x_1 = 10$</br>\n",
    "\n",
    "$3x_0 + x_1 + 5x_2= 22$</br>\n",
    "\n",
    "$32x_0 - 6x_1 -4x_2= 7$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We should probably learn some vocabulary for what we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar\n",
    "\n",
    "$ 2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector $\\vec{v}$\n",
    "\n",
    "$\\begin{bmatrix}8\\\\9\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if I told you that **both** $a$ and $b$ are vectors?\n",
    "\n",
    "$a = \\begin{bmatrix}8\\\\9\\end{bmatrix} \\\\              \n",
    "b = \\begin{bmatrix}8 & 9\\end{bmatrix}$\n",
    "\n",
    "How are they alike?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix\n",
    "$ \\begin{bmatrix}4 & 2 \\\\ 5 & 3 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor\n",
    "\n",
    "$ \\left[ \\begin{array}{ccc} \n",
    "         \\begin{bmatrix}4 & 2 \\\\ 5 & 3 \\end{bmatrix} &\n",
    "         \\begin{bmatrix}6 & -4 \\\\ 2 & 8 \\end{bmatrix} \\\\ \n",
    "         \\begin{bmatrix}-1 & 5 \\\\ 0 & 1 \\end{bmatrix} & \n",
    "         \\begin{bmatrix}9 & -2 \\\\ -5 & 4/5 \\end{bmatrix}  \\end{array} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or put differently:\n",
    "\n",
    "<img src = \"./resources/datadogs.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific definitions of Data Types for Linear Algebra\n",
    "\n",
    "* **Scalars** only have magnitude.\n",
    "\n",
    "* A **vector** is an array with **magnitude and direction**.\n",
    "  - The coordinates of a vector represent where the tip of the vector would be if you travelled from the origin\n",
    "  - The **magnitude** of a vector would be its length in space.\n",
    "\n",
    "* **Matrices** can be interpreted differently in different contexts but it's often used to represent multiple simultaneous vectors. \n",
    "\n",
    "* **Tensors** are made up of matrices with the same dimensions.\n",
    "\n",
    "* A vector or matrix can be multiplied by a scalar to create a change in **scale** and/or **direction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick code break!\n",
    "For linear algebra, `NumPy` is your favorite package.\n",
    "\n",
    "Vectors, matrices and tensors are represented by NumPy arrays. **Not lists!!!** <br>\n",
    "\n",
    "We can use `np.array.shape` to explore the dimensions of these data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make some objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([1, 2, 3, 4, 5, 6])\n",
    "matrix1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "matrix2 = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "tensor = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print them out and find their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector)\n",
    "print('vector shape:', vector.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix1)\n",
    "print('matrix1 shape:', matrix1.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix2)\n",
    "print('matrix2 shape:', matrix2.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor)\n",
    "print('tensor shape:', tensor.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: How would you index or subset a vector, matrix, or tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Index each object to return the **6** for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, let's get back to the....\n",
    "\n",
    "<img src= \"./resources/linear.png\">\n",
    "\n",
    "## part of Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Linear Equations?\n",
    "\n",
    "Linear equations only have **linear variables**. This means our unknowns are only multiplied by a scalar and raised to a power of only **one**, such as:\n",
    "\n",
    "$ x - 2y = 1$\n",
    "\n",
    "$3ex + 2\\pi y = 0$\n",
    "\n",
    "**Not linear:**\n",
    "\n",
    "$ x^2 - 2\\ln{y} = 4$\n",
    "\n",
    "$0.5x + 2y^x = 11$\n",
    "\n",
    "$e^x + 2x=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression built upon Linear Algebra\n",
    "A linear regression can be interpreted as the solution to a system of linear equations: each observation just corresponds to a linear equation, and the **coefficients** are the linear unknowns we're solving for! \n",
    "\n",
    "We're representing each **observation** as a **linear combination of features**.\n",
    "\n",
    "Our prediction equation for a linear regression typically looks something like:\n",
    "\n",
    "$ y_{pred} = \\beta_{0} + \\beta_{1}x_1 + \\beta_{2}x_2 + ... + \\beta_{n}x_n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In matrix notation that can also be:\n",
    "\n",
    "$ y = Xb $, so we are solving for $b$.\n",
    "\n",
    "Where:\n",
    "- $X$ is your matrix of scalars\n",
    "- $b$ is the vector of coefficients\n",
    "\n",
    "Okay, specifically we are solving for $\\hat{b}$:\n",
    "\n",
    "$ \\hat{y} = X\\hat{b}$\n",
    "\n",
    "to:\n",
    "\n",
    "$\\min\\Sigma{(\\hat{y} - y)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pause\n",
    "\n",
    "<img src= \"https://i0.wp.com/timemanagementninja.com/wp-content/uploads/2014/02/Pause-Button-Key.jpg?w=600&ssl=1\">\n",
    "\n",
    "## That was a lot, let's make sure everyone followed with that knowledge drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra powers the majority of machine learning algorithms we will learn in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVPs of Linear Algebra\n",
    "<img src = \"./resources/mvp.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identity Matrix\n",
    "An identity matrix is a square with a diagonal of 1's moving from left to right and the remaining numbers 0. When a matrix is multiplied by an identity matrix, it will result in the same matrix (think of it as the operational equivalent to 1 for linear algebra).\n",
    "\n",
    "<img src = \"./resources/identity_matrix.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_3 = np.identity(3)\n",
    "print(i_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Matrix Inverse\n",
    "The **inverse** of a matrix, when a matrix is multiplied by its inverse, it results in the identity matrix. \n",
    "\n",
    "<img src = \"./resources/inverse.webp\">\n",
    "\n",
    "The order of multiplication does not matter for a matrix and its inverse:\n",
    "\n",
    "$$A \\cdot A^{-1} = A^{-1} \\cdot A $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original matrix\n",
    "x = np.array([[4,8,10],[3,9,12],[5,10,15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse of x and multiplying by x\n",
    "inv_x = np.linalg.inv(x)\n",
    "print(inv_x, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if it produces the identity matrix:\n",
    "print(np.round(x.dot(inv_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix1)\n",
    "print(matrix2)\n",
    "matrix1.dot(matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(matrix2,matrix1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Do all matricies have an inverse?  Nope. \n",
    "\n",
    "    An n-by-n square matrix A is called invertible if there exists an N by N square matrix B such that\n",
    "\n",
    "<div style=\"text-align:center\"><span style=\"color:blue; font-family:Georgia; font-size:1.5em;\">AB = BA = I</span></div>\n",
    "\n",
    "    where I is the identity matrix. A and B are inverses of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait, what was that last function `x.dot` ?\n",
    "### 3. Dot product\n",
    "\n",
    "The dot product of matrices is also commonly known as **Matrix Multiplication**. Unless otherwise stated, _multiplication_ refers to this kind of multiplication.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<img src= \"./resources/matrix_mult.png\" style=\"width: 400px;\">\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html\n",
    "\n",
    "#### Dot product rules:\n",
    "- We take the **rows** (horizontal) of the first matrix and do an element-wise product with the **columns** (vertical) of the second matrix.\n",
    "- Order of operations matters, $AB ≠ BA $  and $(AB)C ≠ A(BC)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Let's do one small dot product by hand! (this is the most matrix math you will be asked to do)\n",
    "\n",
    "$\\begin{bmatrix}8\\\\5\\\\6\\end{bmatrix} * \\begin{bmatrix}3 & 4 & 2 \\end{bmatrix}  = ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transpose\n",
    "\n",
    "The _transpose_ of Matrix $X$, or using notation, $X^{T}$, is matrix $X$ in reverse shape order.\n",
    "\n",
    "$a = \\begin{bmatrix}8\\\\9\\end{bmatrix} \\\\              \n",
    "a^T = \\begin{bmatrix}8 & 9\\end{bmatrix}$\n",
    "\n",
    "Calling `.transpose()` on an array **reverses** the shape order of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original shape of matrix1\n",
    "print(matrix1)\n",
    "print('matrix1 shape:', matrix1.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposed\n",
    "print(matrix1.transpose(), '\\n')\n",
    "print('matrix1.transpose() shape:', matrix1.transpose().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the shorthand function of `.T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "(again, by hand!)\n",
    "\n",
    "What would be the transpose of the following matrix?\n",
    "\n",
    "$\\begin{bmatrix}8 & 2\\\\5 & 3\\\\6&4\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care about these MVPs?\n",
    "![gif](https://media1.giphy.com/media/QA7C1yuI0QZtBbxxM4/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Solves the Best-Fit Line Problem\n",
    "\n",
    "If we have a matrix of predictors $X$ and a target column $y$, we can express $\\hat{y}$, the best-fit line, as  follows:\n",
    "\n",
    "$\\large\\hat{y} = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(list(zip(np.random.normal(size=10),\n",
    "                          np.array(np.random.normal(size=10, loc=2)))))\n",
    "target = np.array(np.random.exponential(size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(preds.T.dot(preds)).dot(preds.T).dot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression(fit_intercept=False).fit(preds, target).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear algebra is used everywhere in machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "We've already covered it. Anywhere you are using a multi-dimensional dataset and optimizing a cost fuction, or transforming the data - linear algebra is how the calculations are run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Text Analytics\n",
    "It is used to model complicated things like language. </br>\n",
    "Some of you may have heard of \"vectorizing text\" when talking about NLP.\n",
    "\n",
    "Converting words and text into vectors and matricies allows us to see how \"close\" and \"far apart\" words are from eachother in meaning and connection.\n",
    "\n",
    "<img src = \"./resources/Word-Vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Image compression and recognition\n",
    "\n",
    "At its basest form, an image is a three dimensional matrix.\n",
    "\n",
    "An $n$ by $m$ by $3$ matrix to be precise.\n",
    "\n",
    "Where $n$ and $m$ are the size of the image and each pixel is an array of three digits represeting its color code.\n",
    "\n",
    "<img src = \"./resources/images.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Recommendation engines \n",
    "Can make much more sophisticated recommendations by using linear algebra in conjunction with user and content data.\n",
    "\n",
    "<img src = \"./resources/netflix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources on Linear Algebra\n",
    "* 3 Blue 1 Brown:  https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_a\n",
    "* Matrix approach to Linear Regression: http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11\n",
    "* [link to fun desmos interaction](https://www.desmos.com/calculator/yovo2ro9me)\n",
    "* [Link to good video on scalars and vectors](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "* [What is X^T * X?](https://stats.stackexchange.com/questions/267948/intuitive-explanation-of-the-xtx-1-term-in-the-variance-of-least-square/267963)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.desmos.com/calculator/y08wwbjwid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"./images/grad-desc-text.png\" style = \"width: 600px;\">\n",
    "\n",
    "<img src= \"./images/muchmath.png\" style = \"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main goal\n",
    "Enough to conceptually _explain_ it, no expectation to _recode it from scratch_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Linear Regression\n",
    "### Remember, Linear albegra turns these equations:\n",
    "\n",
    "$4x_1 + 2x_2 = 8$</br>\n",
    "\n",
    "$5x_1 + 3x_2 = 9$\n",
    "\n",
    "### into this:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}4 & 2 \\\\ 5 & 3 \\end{bmatrix}*\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} = \\begin{bmatrix}8\\\\9\\end{bmatrix}\n",
    "$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In linear regression using linear algebra:\n",
    "\n",
    "We're representing each **observation** as a **linear combination of features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we were using an Auto dataset to predict **mpg** from *cylinders, displacement, horsepower, weight, acceleration and year*, we see the data like this:\n",
    "\n",
    "\n",
    "![data](./images/data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we know we want to find a linear regression formula that looks like:\n",
    "\n",
    "$ y_{pred} = \\beta_{0} + \\beta_{1}x_1 + \\beta_{2}x_2 + ... + \\beta_{n}x_n $\n",
    "\n",
    "\n",
    "### How does the computer represent the observations and betas in the form of linear algebra?\n",
    "\n",
    "#### Exercise: write it out using what we have above\n",
    "\n",
    "\n",
    ".. I'll wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge:\n",
    "\n",
    "Okay, so how do we find the betas? How do we find the coefficients?\n",
    "\n",
    "Let's start with a more simple example and come back to our mpg later.\n",
    "\n",
    "\n",
    "\n",
    "We've learned previously that for regression we find the minimum sum of squares:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression and Sum of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating some data to make a mini-regression problem\n",
    "#### This will be quick, it's more about the last graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 1, 2, 3, 4, 3, 4, 6, 4]\n",
    "y = [2, 1, 0.5, 1, 3, 3, 2, 5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Playing around with parameters to find the line of best fit\n",
    "#### We know in theory how to find the best SINGLE coefficient - change it around and see what works!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$h_\\theta(x) = \\theta_0 + \\theta_1 x $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$ Y = \\beta_0  + \\beta_1 x $</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1 = [.25, .5, .75, .8, 1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying different values of $b_1$ to see which works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating predictions and cost function values\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "mses = []\n",
    "for t in beta_1:\n",
    "    line = beta_0 + (np.array(x)*t)\n",
    "    # Our cost function\n",
    "    mse = round(mean_squared_error(y, line),3)\n",
    "    mses.append(mse)\n",
    "    ax.plot(x, line, label=f'{mse} {t}')\n",
    "    \n",
    "ax.scatter(x,y)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cost Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(beta_1, mses)\n",
    "ax.set_title('Cost Curve')\n",
    "ax.set_xlabel('beta 1')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that a $b_1$ around 7.5 works best.\n",
    "\n",
    "**QUICK CHECK**:  What do I mean by \"Best\"??? </br>\n",
    "\n",
    "## Challenge\n",
    "**AND**:  How do we do this for more than one coefficient at the same time??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT DECENT to the RESCUE!!\n",
    "\n",
    "![costcurve](./images/Gradie_Desce.jpg)\n",
    "\n",
    "\n",
    "#### **QUESTION** - What's the third axis?\n",
    "#### **another QUESTION** - What's that curvy thing?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Axis:  Loss, cost and objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review: What is cost?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$MSE = \\frac{1}{m}\\sum_{i=1}^m (Y_i - \\hat Y_i)^2 $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: How do we find the lowest points?\n",
    "\n",
    "### Option 1: Point and pick.\n",
    "We have a graph. Pick it!<br>\n",
    "Anyone see any problems with that? Would that work in more than two dimensions?\n",
    "\n",
    "### Option 2: do the math for every point and do a min() function\n",
    "Okay, also an option. <br>\n",
    "Anyone see any issues with data storage? Time to process?\n",
    "\n",
    "### Option 3: A quicker way. Math.\n",
    "Let's use some calculus!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-align:center'>Calculus</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the steps we take to get to the \"bottom\" of the curve? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/ralph.gif'/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an <b>algorithm</b> used to find the lowest point of a function (remember we are talking about our \n",
    "**cost function** here). </br>\n",
    "\n",
    "It is a process that helps us **change** our parameters (coefficients) <br>\n",
    "until we get the **optimal** parameters( again, coefficients) of our function<br>\n",
    "(the **lowest** value of our cost function). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick concept - \n",
    "\n",
    "In the diagram above, there are TWO parameters ( $b_0$ for $x_0$  and $b_1$ for $x_1$ ), so our **cost function** is in a THREE dimensional space. \n",
    "\n",
    "The cost function will always be a plane in an $n+1$ dimesional space, where $n$ is the number of coefficients you are trying to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we go about changing our parameters? \n",
    "\n",
    "The Steps:\n",
    "\n",
    "    - Start with your parameters at any arbitrary value\n",
    "    - Find the gradient at that point (how steep is the curve) \n",
    "    - Multiply Learning Rate * Gradient - to calculate the amount you want to change \n",
    "    - Subtract above value from current parameter to create new parameter estimate\n",
    "    - Repeat until gradient is ~ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In theory, the process of finding the optimal coefficients will look something like this:\n",
    "\n",
    "<img src= \"https://thumbs.gfycat.com/GentleEquatorialDove-size_restricted.gif\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Pick a random value of one of the coefficients\n",
    "\n",
    "<img src='./images/gradient_desc-step1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Find the _gradient_ at that point on the curve\n",
    "![slope](./images/gradient_desc_step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADIENT\n",
    "\n",
    "The gradient is just **derivatives**. </br>\n",
    "The **derivative** of any point on a **curve** is the **slope** at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Change the value of the coefficient by a set amount (_learning rate_)\n",
    "They give us how much things change when we make a tiny step.<br>\n",
    "\n",
    "![slope](./images/gradient_desc_step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do we choose weight A or weight B?\n",
    "#### In which direction do we go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Change the value of the coefficient to Option A\n",
    "You want to pursue the lower gradient.<br>\n",
    "\n",
    "![slope](./images/gradient_desc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Find the place on the curve where the derivative is zero\n",
    "#### For example, at each labeled point on this curve, the derivative is zero.\n",
    "\n",
    "![minmax](./images/maxandmin.png)\n",
    "\n",
    "\n",
    "The derivative of a function at its lowest point is **zero**<br>\n",
    "The closer we get to the lowest point the lower our derivative will be<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if our steps are too big and we miss the minima?\n",
    "![alt text](./images/bigsteps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change step size as derivative gets smaller\n",
    "\n",
    "When the derivative is **large** we will change our parameters/coefficients **by more** to speed up the process<br>\n",
    "\n",
    "When the derivative is **small** we will change our parameters/coefficients **by less** to not overshoot the optimal value<br>\n",
    "\n",
    "#### Take baby steps.\n",
    "\n",
    "![baby](https://i.imgur.com/oIK0FyE.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Parameters?\n",
    "#### only small change in approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with multiple parameters we need to use partial derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='width:500px' src='./images/gradient.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/RSSbowl.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each partial derivative tells us how much a small change in the parameter will affect the cost. Same as before.<br>\n",
    "Now we know how much we need to change our parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate helps us \"soften\" the amount of change we apply to a parameter?</br> Why might we want to do this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get out those laptops!\n",
    "#### Exercise:\n",
    "\n",
    "We are going to divide the room into three sections:\n",
    "- small, learning rate = 0.05\n",
    "- medium, learning rate = 1.4\n",
    "- large, learning rate = 3.5\n",
    "\n",
    "Working in pairs, to use this website: </br>\n",
    "https://developers.google.com/machine-learning/crash-course/fitter/graph\n",
    "\n",
    "And do the following steps:\n",
    "- set the learning rate to your group's learning rate\n",
    "- click \"step\" as many times as it takes to reach the blue star\n",
    "- report out on how many steps it took to reach the global min and take note of any strange behavior.\n",
    "\n",
    "I'll give you a few minutes to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report out:\n",
    "- people with the small learning rate, how many steps did it take?\n",
    "- how about medium?\n",
    "- how about large?\n",
    "\n",
    "What behavior did you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of iteratively changing the coefficients can be described with this formula:\n",
    "\n",
    "<img stycle='width:700px' src='./images/GD_formula.png' />\n",
    "\n",
    "$ \\alpha $ represents the learning rate and chosen before you start the \"learning\" process\n",
    "\n",
    "$ \\theta_0 $ is the coefficient you are adjusting\n",
    "\n",
    "$ J(\\theta) $ is the derivative used to find the slope of the cost function at any value of the coefficient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pause and make sure we are all stilll together\n",
    "\n",
    "![pause](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSE-Q6G4qkF9u1tg9qYYHpf0JwMb_uuaiL6m8p6wfimqkAgOcVb8w&s)\n",
    "\n",
    "Quick **Fist of five** - show me all 5 fingers if you absolutely understand all of this, zero if you are complete lost, and any corresponding number in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems With Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 1**: Local Minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local minima occur when there are multiple minimum points in your cost function :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img stycle='width:700px' src='./images/minima.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on how to address this problem?\n",
    "let's take a few thoughts from the class.</br>\n",
    "\n",
    ".</br>\n",
    "\n",
    ".</br>\n",
    "\n",
    ".</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: run it multiple times with different starting points to see if same coefficients are found.\n",
    "\n",
    "This is also why we set the seed in some exercises to ensure we all get the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2**: Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep repeating the steps above until your parameters stop changing aka there is no more gradient. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1: doing the maths out, for those who love the maths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From our original cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;font-size:20px'>$MSE = \\frac{1}{m}\\sum_{i=1}^m (\\hat Y_i - Y_i)^2 $</p>\n",
    "<p style='text-align:center;font-size:20px'>$\\hat Y = \\theta_0 + \\theta_1 x $</p>\n",
    "<p style='text-align:center;font-size:20px'>$MSE = \\frac{1}{m}\\sum_{i=1}^m (\\theta_0 + \\theta_1 x_i - Y_i)^2 $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the chain rule in each partial derivative<br>\n",
    "<p style='text-align:center;font-size:20px'>$ J(\\theta_0, \\theta_1) = \\frac{1}{m}\\sum_{i=1}^m (\\theta_0 + \\theta_1 x_i - Y_i)^2 $</p>\n",
    "\n",
    "When derivating on the intercept:\n",
    "<p style='text-align:center;font-size:20px'>$ \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial\\theta_0} = \\frac{2}{m} \\sum_{i=1}^m \\theta_0 + \\theta_1 x_i - Y_i$<br>\n",
    "\n",
    "When derivating on a weight:\n",
    "<p style='text-align:center;font-size:20px'>$ \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial\\theta_1} = \\frac{2}{m} \\sum_{i=1}^m (\\theta_0 + \\theta_1 x_i - Y_i) * x_i $\n",
    "\n",
    "Or from an algebra / matricial perspective:\n",
    "\n",
    "<p style='text-align:center;font-size:20px'>$ \\frac{dJ(\\theta)}{d\\theta} = \\frac{2}{m}  X^T*LossVector $\n",
    "    \n",
    "![alt text](./images/vectorofpartialderivative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2: Gradient descent in four lines of code (the core anyway...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.tools import add_constant\n",
    "import seaborn as sns\n",
    "\n",
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    x = add_constant(x)\n",
    "    xTrans = x.T\n",
    "    costs = []\n",
    "    for i in range(0, numIterations):\n",
    "        preds = x@theta\n",
    "        loss = preds - y\n",
    "        gradient = xTrans@loss * (2 / m)\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "    # Showing cost reduction overtime\n",
    "        cost = np.sum(loss ** 2) / m\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "            print(theta)\n",
    "            costs.append(cost)\n",
    "    sns.scatterplot(y = costs, x = [i for i in range(len(costs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(x) \n",
    "numIterations= 10000\n",
    "alpha = 0.0001\n",
    "theta = np.array([1.90595185,1.5342646])\n",
    "gradientDescent(x, y, theta, alpha, m, numIterations) # [ 0.28623922 46.7549264 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 3: Speed comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,c = make_regression(1000000,2,random_state=1000,coef=True)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(x, y)\n",
    "print(f'theta_0: {ols.intercept_}')\n",
    "print(f'theta: {ols.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gd = linear_model.SGDRegressor(alpha=.0001, max_iter=1000, tol=None\n",
    "                              )\n",
    "gd.fit(x,y)\n",
    "print(f'theta_0: {gd.intercept_[0]}')\n",
    "print(f'theta_1: {gd.coef_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Understanding the Mathematics Behind Gradient Descent](https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
