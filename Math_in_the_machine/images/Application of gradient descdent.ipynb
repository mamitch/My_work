{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shower Questions:\n",
    "\n",
    "### In machine learning what is the difference between an  algorithm and a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Gradient Descent\n",
    "\n",
    "### Aim: Students will be able to identify modeling problems in which they should utilize Gradient Descent to solve the equation, and be able to implement a GD algorithm for a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the chain rool to MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives of the cost function\n",
    "\n",
    "![alt text](gdformula.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient vector of the cost function\n",
    "Results in doble the mean of XT.loss\n",
    "\n",
    "![alt text](vectorofpartialderivative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Gradient Descent* is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](findingminimum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](bigsteps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RSSbowl.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](contourplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When doing GD it is important to scale your features to a similiar scale or else it will take the algorithm longer to converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](scalecontour.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient vector of the cost function\n",
    "\n",
    "![alt text](vectorofpartialderivative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "\n",
    "* This formula involves caluclations over the full training set X, at each Gradient Descent step. \n",
    "* Terribly slow on large number of observations, but scales well with the number of features.\n",
    "* When training a Linear Regression model with hundreds of thousands of features GRadient DEscent is much faster than the Normal Equation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Picks a random instance in the training set at every step and computes the gradients based only on that single instance. \n",
    "\n",
    "Algorithm is much faster given it has very little data to manipulate. \n",
    "\n",
    "Can be used as an out-of-core algorithm, since it only needs one data observation at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cost function is very irregular, SGD has a better chance of of finding the global minimum than BGD.\n",
    "This could be bad because the algorithm will never settle at a minimum.  \n",
    "One way to avoid this is by reducing the learning rate (step size) as the algorithm progresses. \n",
    "\n",
    "The function that determines the learning rate at each step is called the *learning schedule*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent\n",
    "\n",
    "At each step, computing the gradietns is based on a small random set of instaces called *mini-batches*.\n",
    "\n",
    "Mini-batch GD will be less erratic than SGD because it is training on a multiple data instances. \n",
    "\n",
    "Faster than Batch GD.\n",
    "\n",
    "Harder to escape local minima than SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](comparegdtypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](algotable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](albotable2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
