{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "<img src= \"img/pipelines.png\" style=\"height:450px\">\n",
    "\n",
    "\n",
    "[Image Source](https://towardsdatascience.com/using-functiontransformer-and-pipeline-in-sklearn-to-predict-chardonnay-ratings-9b13fdd6c6fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Objectives\n",
    "\n",
    "By the end of the lesson students will be able to:\n",
    "- Summarize the purpose of pipelines\n",
    "- Implement a scikit-learn pipeline to modularize a modeling process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to modeling we have learned so far:\n",
    "\n",
    "1. Preprocess our data\n",
    "    - scaling\n",
    "    - imputing missing values\n",
    "  \n",
    "\n",
    "2. Fitting our model to our training data\n",
    "\n",
    "3. Predicting new values with our test data\n",
    "\n",
    "4. Obtaining metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how we would use a logistic regression model to predict breast cancer using the steps we have learned so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our data\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "X.head()\n",
    "y=pd.Series(cancer.target)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting our data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale our training data\n",
    "ss=StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "\n",
    "#scale our test data\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "#fit our logistic regression model\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "#examine the accuracy of our model\n",
    "accuracy = log_reg.score(X_test_scaled, y_test)\n",
    "print(f'Accuracy Score:{accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `sklearn.pipeline.Pipeline()` object\n",
    "\n",
    "![kid transformer](./img//transformer.gif)\n",
    "\n",
    "### Definition of a pipeline\n",
    "\n",
    "We'll store these steps in a `Pipeline` object. From the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn-pipeline-pipeline):\n",
    "\n",
    "> The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. [The `Pipeline` object] sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’ (i.e. transformers), that is, they must implement fit and transform methods. \n",
    "\n",
    "### Broad generalization of a pipeline\n",
    "\n",
    "The key here is we need to specify a specific column, pass it's \"transformer\" (i.e. `SimpleImputer`, `OneHotEncoder`, `StandardScaler`), and determine if the transformation belongs in it's own new column or if it's more appropriate for the transformed column to overwrite the input column.\n",
    "\n",
    "### Benefits of the `Pipeline`\n",
    "\n",
    "From the [User Guide](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators):\n",
    "> * **Convenience and encapsulation**\n",
    ">     + You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "> * **Joint parameter selection**\n",
    ">     + You can grid search over parameters of all estimators in the pipeline at once.\n",
    "> * **Safety**\n",
    ">     + Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors. \n",
    "\n",
    "#### See these resources on data leakage\n",
    "- [Kaggle Data Leakage](https://www.kaggle.com/alexisbcook/data-leakage)\n",
    "-[Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/)\n",
    "- [Leakage in Data Mining: Formulation, Detection, and Avoidance](https://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf)\n",
    "\n",
    "\n",
    "### BONUS: We can create custom transformers\n",
    "\n",
    "Sometimes we need to add new features to our existing feature space. In this case, we can't rely on importing a traditional transformer (i.e. `StandardScaler`, `OneHotEncoder`, etc.).\n",
    "\n",
    "Instead, we'll need to create our own custom transformer. We can do this by creating a new class that implements both `.fit()` and `.transform()` methods. \n",
    "\n",
    "*See [Sebastian Raschka](https://sebastianraschka.com/) to see how you can do this!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline\n",
    "\n",
    "Here we can create a simple pipeline to do the standard scaling and modeling steps that we performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steps = [('ss', StandardScaler()), ('log_reg', LogisticRegression())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "pipe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Create your own pipeline. Chose a transformers and an estimator with given hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great!  Now that we have our pipeline let's use it to make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting our data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of:\n",
    "minmax = MinMaxScaler()\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn also gives us \"make_pipeline\" which is almost the same thing but with make_pipeline you don't have to give names.\n",
    "\n",
    "__Your Turn__\n",
    "\n",
    "-  [Check documentation: 6.1.1.1.1. Construction](https://scikit-learn.org/stable/modules/compose.html) and use make_pipeline to construct an pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing steps\n",
    "\n",
    "We have multiple ways to access and object in the pipeline\n",
    "\n",
    "- steps attribute\n",
    "\n",
    "- [idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note that these will all give the minmax scaler object\n",
    "\n",
    "# pipe.steps[0][1]\n",
    "\n",
    "# pipe['minmax']\n",
    "\n",
    "# pipe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also access a particular object by named_steps\n",
    "## sklearn claims that tab completion should work here but \n",
    "## in my notebook it didn't\n",
    "\n",
    "pipe.named_steps.minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can 'slice' pipelines to create sub-pipes\n",
    "\n",
    "pipe[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipe.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the parameters\n",
    "\n",
    "Parameters of the estimators in the pipeline can be accessed using the \n",
    "\"estimator__parameter\" syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['minmax'].get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(knn__n_neighbors=6,\n",
    "                knn__leaf_size=15,)\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming target in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_boston(return_X_y=True)\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "regressor = LinearRegression()\n",
    "regr = TransformedTargetRegressor(regressor=regressor,\n",
    "                                  transformer=transformer)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\n",
    "\n",
    "raw_target_regr = LinearRegression().fit(X_train, y_train)\n",
    "print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/diabetes.csv')\n",
    "display(df.head(), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.Outcome\n",
    "\n",
    "data = df.drop(columns='Outcome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[On Scaling data](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                    target,\n",
    "                                                    test_size=0.20,\n",
    "                                                    stratify=target,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Create a pipeline and use this pipeline for fitting and predicting diabetes results for the above data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Transformers\n",
    "\n",
    "What happens when you are preparing your data but it has mixed data types?\n",
    "\n",
    "Have you been splitting your dataframe, transforming, and then merging them back together???\n",
    "\n",
    "![](./img/yes-well-not-anymore.jpg)\n",
    "\n",
    "Sklearn has a really nice pipeline tool for this call [ColumnTransformer](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Create a column transformer that will standard scale all continuous variables and one hot encode any discrete variables using the termination dataset.\n",
    "\n",
    "\n",
    "__Bonus!  Can you use this column transformer inside a pipeline to make predictions about employee terminations?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further research and miscellaneous\n",
    "\n",
    "- [FeatureUnion](https://scikit-learn.org/stable/modules/compose.html#featureunion-composite-feature-spaces)\n",
    "\n",
    "- [ColumnTransformer](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)\n",
    "\n",
    "- [sklearn, dictionary of terms](https://scikit-learn.org/stable/glossary.html#term-transformer)\n",
    "\n",
    "- [Pydata meeting on pipelines](https://www.youtube.com/watch?v=BFaadIqWlAg)\n",
    "\n",
    "- [Another pydata talk on pipelines with FeatureUnion](https://www.youtube.com/watch?v=URdnFlZnlaE)\n",
    "\n",
    "- [On scalers](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler)\n",
    "\n",
    "- [A nice notebook on pipelines](https://github.com/amueller/introduction_to_ml_with_python/blob/master/06-algorithm-chains-and-pipelines.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
