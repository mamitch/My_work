{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "### We use them every day\n",
    "\n",
    "![tree](img/tree.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of the lesson students will be able to:\n",
    "- Summarize the intuitive logic behind decision trees\n",
    "- Describe how we can use Gini-Entropy measures for measuring the pureness of a node.\n",
    "- Identify Pros and cons of the decision trees\n",
    "- Identify hyperparameters we can adjust in `sklearn` for decision trees\n",
    "- build a decision tree in `sklearn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The key tool of decision trees is that some attributes provide more _information_ than others when trying to make a decision.<br>\n",
    "And we rank attributes in the hierarchy based on how useful they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1 - when looking to monogamously date someone\n",
    "Which is more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Their taste in music?\n",
    "![music](img/music.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or if they are married already?\n",
    "![married2](img/married2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2 - when looking for someone to pet sit your cat\n",
    "Which is more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How many cats **they** have:\n",
    "\n",
    "![cats](img/cats.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How many cats have **died** on their watch while pet sitting:\n",
    "\n",
    "![petcem](img/petcem.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 3 with some vocab\n",
    "Rory is a teenager trying to decide if he wants to go to a party, and this is the decision tree represtinging his process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### His decision tree\n",
    "![party](img/party.png)\n",
    "\n",
    "( image from this [site](https://chunml.github.io/ChunML.github.io/tutorial/Decision-Tree/) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary: nodes\n",
    "\n",
    "![nodes](img/terminology1.png)\n",
    "\n",
    "**Root Node**: It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "\n",
    "**Decision Node**: When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "\n",
    "**Leaf/ Terminal Node**: Nodes do not split is called Leaf or Terminal node.\n",
    "\n",
    "**Parent and Child Node:** A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary: splits & branches\n",
    "\n",
    "![split](img/terminology2.png)\n",
    "\n",
    "**Splitting**: It is a process of dividing a node into two or more sub-nodes.\n",
    "\n",
    "**Branch / Sub-Tree:** A sub section of decision tree is called branch or sub-tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 4: with data\n",
    "\n",
    "Suppose we're working on a classification algorithm designed to **sort customers into two classes: those who pay their sales bills and those who don't**.\n",
    "\n",
    "Each row in my dataframe represents a customer, and I have many predictors (columns) in my dataframe, including:\n",
    "\n",
    "- salary\n",
    "- total_bill\n",
    "- club_member (boolean)\n",
    "- years_post-sec_ed\n",
    "\n",
    "Let's look at a simple set of data. **The 'paid' column is our target or dependent variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "custs = pd.DataFrame([[45000, 1000, True, 2, False], [70000, 100, True, 10, True],\n",
    "             [30000, 2000, False, 0, False], [90000, 500, True, 2, True],\n",
    "             [70000, 200, True, 5, False]],\n",
    "            columns=['salary', 'total_bill', 'club_member', 'years_post-sec_ed',\n",
    "                    'paid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partitioning\n",
    "\n",
    "I partition my data by asking a question about the independent variables. The goal is to ask the right questions in the right order so that the resultant groups are \"pure\" with respect to the dependent variable. More on this below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's explore some variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test = custs.sort_values(['salary'])\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is the customer a club member?\n",
    "\n",
    "This would divide my data into two groups (one is my yellow highlighted rows and the other is aqua):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_split(x):\n",
    "    if x.club_member == True:\n",
    "        return ['background-color: yellow']*5\n",
    "    else:\n",
    "        return ['background-color: aqua']*5 \n",
    "test.style.apply(highlight_split, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Group 1 (Yellow Group):**\n",
    "\n",
    "- 2 customers who paid 2 customers who did not\n",
    "\n",
    "**Group 2 (Aqua Group):**\n",
    "\n",
    "- One customer who did not\n",
    "\n",
    "While I've isolated one of the customers who haven't paid in the second group, the first group is an even mix of payers and non-payers. So this split is not particularly good.\n",
    "\n",
    "Would a different question split our data more effectively? Let's try:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Is the customer's salary less than $60k?\"\n",
    "\n",
    "Again we divide the data into two groups (Yellow are the customers who have a salary less than $60,000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_split(x):\n",
    "    if x.salary < 60000:\n",
    "        return ['background-color: yellow']*5\n",
    "    else:\n",
    "        return ['background-color: aqua']*5 \n",
    "test.style.apply(highlight_split, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Group 1 (Yellow Group):**\n",
    "\n",
    "- 2 customers who paid 2 customers who did not\n",
    "\n",
    "**Group 2 (Aqua Group):**\n",
    "\n",
    "- One customer who did not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which feature is more accurate in predicting whether and individual pays their bill?  How do you know this??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy and Information Gain\n",
    "\n",
    "The goal is to have our ultimate classes be fully \"ordered\" (for a binary dependent variable, we'd have the 1's in one group and the 0's in the other). So one way to assess the value of a split is to measure how *disordered* our groups are, and there is a notion of *entropy* that measures precisely this.\n",
    "\n",
    "**Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a given split, the **information gain** is simply the entropy of the parent group less the entropy of the split.\n",
    "\n",
    "For a given parent, then, we maximize our model's performance by *minimizing* the split's entropy.\n",
    "\n",
    "What we'd like to do then is:\n",
    "\n",
    "1. to look at the entropies of all possible splits, and\n",
    "2. to choose the split with the lowest entropy.\n",
    "\n",
    "In practice there are far too many splits for it to be practical for a person to calculate all these different entropies ...\n",
    "\n",
    "... but we can make computers do these calculations for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gini Impurity\n",
    "\n",
    "An alternative metric to entropy which measures the **degree or probability of a particular variable being wrongly classified** when it is randomly chosen. But what is actually meant by ‘impurity’? If all the elements belong to a single class, then it can be called pure. \n",
    "\n",
    "The degree of Gini index varies between 0 and 1, where 0 denotes that all elements belong to a certain class or if there exists only one class, and 1 denotes that the elements are randomly distributed across various classes. A Gini Index of 0.5 denotes equally distributed elements into some classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which is better?\n",
    "\n",
    "Gini impurtity and information gain (entropy) will perform the same. However, entropy requires you to compute logarithmic functions, which are computationally expensive so Gini might be preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/gini_entropy_chart.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As found in *Introduction to Data Mining* by Tan et. al:\n",
    "\n",
    "`Studies have shown that the choice of impurity measure has little effect on the performance of decision tree induction algorithms. This is because many impurity measures are quite consistent with each other [...]. Indeed, the strategy used to prune the tree has a greater impact on the final tree than the choice of impurity measure.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steps in the Decision Tree Model:\n",
    "\n",
    "1. Find the best predictor of the output (e.g. using information gain criterion) and make it a root node of the decision tree.\n",
    "2. Split the data into disjoint subsets containing unique values of the selected input.\n",
    "\n",
    "3. For each of the subsets: Recurse from (1) while further splitting to completely pure nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coding Trees in Python\n",
    "\n",
    "Scikit-learn has a tree module, which houses both a DecisionTreeClassifier and a DecisionTreeRegressor. The difference, as is probably clear by now, is that the former is for classification problems (discrete target) and the latter is for regression problems (continuous target). Let's use the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ct = DecisionTreeClassifier() # Check out all the\n",
    "                              # hyperparameter options here!\n",
    "\n",
    "dt_mod = ct.fit(custs.drop('paid', axis=1), custs['paid'])\n",
    "dt_mod.score(custs.drop('paid', axis=1), custs['paid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow we predicted with 100% Accuracy but let's look at some other metrics!\n",
    "\n",
    "## Your Turn!  \n",
    "\n",
    "Create a confusion matrix, classification report, report the predicted probabilities for each item in the dataset, plot a ROC Curve, and print the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing our Decision tree\n",
    "\n",
    "We can also visualize our decision tree using the `plot_tree` function in the tree module.  Let's take a look at our tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fn=['salary', 'total_bill', 'club_member', 'years_post-sec_ed']\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (2,3), dpi=300)\n",
    "tree.plot_tree(dt_mod, feature_names=fn, class_names=['False', 'True'], filled=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice about the leaves of our tree?  What has our model ensured?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Hyperparameters\n",
    "\n",
    "To avoid overfitting our data we can \"prune\" our tree by adjusting our hyperparameters\n",
    "- `min_samples_split` : the minimum number of samples a note must have before it can be split\n",
    "- `min_samples_leaf` : the minimum numbers of samples a leaf node must have\n",
    "- `max_leaf_nodes` : the maximum number of leaf nodes\n",
    "- `max_features` : the maximum number of features that are evaluated for splitting at each node\n",
    "- `max_depth`: the maximum depth the tree can go\n",
    "- `min_impurity_split` : a threshold of impurity required to split a node\n",
    "\n",
    "\n",
    "Choose 2 hyperparameters to adjust and rerun your model again.  Were you able to prevent the \"perfect fit\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pros and Cons of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pros:\n",
    "- Easy to Understand\n",
    "- Useful in Data exploration\n",
    "- Can capture non-linear patterns\n",
    "- Does not require as many preprocessing steps\n",
    "- Less data cleaning required\n",
    "- Data type is not a constraint\n",
    "- Non Parametric Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Cons:\n",
    "- Prone to over-fitting\n",
    "- Sensitive to noisy data\n",
    "- Small variations in data can greatly influence the algorithm\n",
    "- Struggles with creating cut-off splits with continuous variables\n",
    "- Biased with imbalanced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time to Practice!\n",
    "\n",
    "1. Using the `Vehicle_loan_default` dataset create a vanilla decision tree classifier that predicts `loan_default`.  Use at least 8 features!\n",
    "\n",
    "2. Now, change at least 2 of your hyperparameters of your decision tree model.  Compare the metrics of both your models.  Which one does better?\n",
    "\n",
    "3.  BONUS!  Create a KNN model to predict `loan_default`.  Which model does better, KNN or decision tree?  With what hyperparameters? What metric did you use to compare?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating Entropy\n",
    "\n",
    "The entropy of the whole dataset is given by:\n",
    "\n",
    "$\\large E = -\\Sigma_i p_i\\log_2(p_i)$,\n",
    "\n",
    "where $p_i$ is the probability of belonging to the $i$th group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To repeat, in the present case we have only two groups of interest: the payers (2/5) and the non-payers (3/5).\n",
    "\n",
    "So our entropy for this toy dataset is:\n",
    "\n",
    "$-0.4*\\log_2(0.4) -0.6*\\log_2(0.6)$.\n",
    "\n",
    "Let's use the ```math``` library to calculate this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To calculate the entropy of a *split*, we're going to want to calculate the entropy of each of the groups made by the split, and then calculate a weighted average of those groups' entropies––weighted, that is, by the size of the groups. Let's calculate the entropy of the split produced by our question above about salary:\n",
    "\n",
    "Group 1:\n",
    "\n",
    "$E_{g1} = 0 * \\log_2(0) - 1 * \\log_2(1) = 0$. This is a pure group! The probability of being a payer in Group 1 is 0 and the probability of being a non-payer in Group 1 is 1.\n",
    "\n",
    "Group 2:\n",
    "\n",
    "$E_{g2} = \\frac{2}{3} * \\log_2\\left(\\frac{2}{3}\\right) - \\frac{1}{3} * \\log_2\\left(\\frac{1}{3}\\right)$.\n",
    "\n",
    "Once again, using ```math```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get the whole entropy for this split, we'll do a weighted sum of the two group entropies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper Dive on Entropy\n",
    "- https://towardsdatascience.com/demystifying-entropy-f2c3221e2550\n",
    "- https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gini Impurity\n",
    "\n",
    "An alternative metric to entropy comes from the work of Corrado Gini. The Gini Impurity is defined as:\n",
    "\n",
    "$\\large G = 1 - \\Sigma_i p_i^2$,\n",
    "\n",
    "where, again, $p_i$ is the probability of belonging to the $i$th group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great explanation on the derivation fo the Gini index.\n",
    "\n",
    "https://www.quora.com/What-is-the-interpretation-and-intuitive-explanation-of-Gini-impurity-in-decision-trees"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
